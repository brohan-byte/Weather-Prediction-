{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48081c54-eb3d-4ffc-a49f-2253fcb2571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME = /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\n",
      "java -version ->\n",
      "openjdk version \"17.0.16\" 2025-07-15\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.16+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.16+0, mixed mode, sharing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, json\n",
    "\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"JAVA_HOME =\", os.environ.get(\"JAVA_HOME\"))\n",
    "print(\"java -version ->\")\n",
    "print(subprocess.check_output([\"bash\",\"-lc\",\"java -version\"], stderr=subprocess.STDOUT).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214eaa11-6d1d-4d92-9048-03aacdbf020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col, month, date_add, to_date,from_json\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f82960c-7a77-4e57-85b1-0fbd973ab320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.16\" 2025-07-15\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.16+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.16+0, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5581b567-c25c-489b-bb3d-8a3b989d36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/Users/rohan/Desktop/CS/projects/kafka_project/venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/rohan/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/rohan/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8446878c-0ec4-4fcd-90f5-895681d2895a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 165ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8446878c-0ec4-4fcd-90f5-895681d2895a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/4ms)\n",
      "25/08/29 11:52:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "      .appName(\"\")\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "      .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "      .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0\")\n",
    "      .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b48d37-9f84-4023-9290-2713b925c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef18ffc1-7b98-416e-a4c6-87a61d7d7dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db964d7-c27b-4549-bc87-b4a76d95dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"station STRING, date DATE, degrees DOUBLE, raining INT\"\n",
    "\n",
    "records = (\n",
    "    df.select(col(\"key\").cast(\"string\").alias(\"key\"),\n",
    "              col(\"value\").cast(\"string\").alias(\"json\"))\n",
    "      .select(\"key\", from_json(col(\"json\"), schema).alias(\"value\"))\n",
    "      .select(\"key\", \"value.*\")   # expands to date, station, degrees, raining\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa98900-dc28-4f46-8a8c-02a5f25637e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 11:52:45 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/5r/89jndgws4q39x08qhlxlm_gr0000gn/T/temporary-46210cf7-6106-4804-8a55-1181b508bb6b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/29 11:52:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min as fmin, max as fmax, avg as favg, count as fcount\n",
    "stats = (records.groupBy(\"station\")\n",
    "           .agg(\n",
    "               fmin(\"date\").alias(\"start\"),\n",
    "               fmax(\"date\").alias(\"end\"),\n",
    "               fcount(\"*\").alias(\"measurements\"),\n",
    "               favg(\"degrees\").alias(\"avg\"),\n",
    "               fmax(\"degrees\").alias(\"max\")\n",
    "           )\n",
    "           .select(\"station\", \"start\", \"end\", \"measurements\", \"avg\", \"max\")  # order columns\n",
    "           .orderBy(\"station\"))\n",
    "\n",
    "q_stats = (stats.writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\")            # print full table every trigger\n",
    "         .trigger(processingTime=\"5 seconds\")\n",
    "         .option(\"truncate\", \"false\")\n",
    "         .option(\"numRows\", 1000)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11c8657-296f-4248-9e41-4240a0b79be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 11:52:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/08/29 11:52:48 ERROR FileFormatWriter: Aborting job c70260b4-527f-401a-814a-fb6ff967a02b.\n",
      "org.apache.spark.sql.kafka010.KafkaIllegalStateException: Some data may have been lost because they are not available in Kafka any more;\n",
      "either the data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      "topic was processed.\n",
      "If you don't want your streaming query to fail on such cases, set the source option failOnDataLoss to false.\n",
      "Reason: Partition stations-json-2 offset was changed from 453 to 33.\n",
      "\tat org.apache.spark.sql.kafka010.KafkaExceptions$.partitionOffsetChanged(KafkaExceptions.scala:151)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$10(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.reportDataLoss(KafkaMicroBatchStream.scala:311)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1(KafkaMicroBatchStream.scala:203)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1$adapted(KafkaMicroBatchStream.scala:203)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$9(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderAdmin.scala:501)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:203)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.$anonfun$partitions$3(DataSourceV2ScanExecBase.scala:67)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions(DataSourceV2ScanExecBase.scala:67)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions$(DataSourceV2ScanExecBase.scala:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions(MicroBatchScanExec.scala:31)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD$lzycompute(MicroBatchScanExec.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD(MicroBatchScanExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute(DataSourceV2ScanExecBase.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute$(DataSourceV2ScanExecBase.scala:185)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.doExecute(MicroBatchScanExec.scala:31)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec.doExecute(EventTimeWatermarkExec.scala:106)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:209)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:209)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:245)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:243)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:260)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$1(FileFormatWriter.scala:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:211)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:176)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:879)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaExceptions$.partitionOffsetChanged(KafkaExceptions.scala:151)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$10(KafkaOffsetReaderAdmin.scala:509)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.reportDataLoss(KafkaMicroBatchStream.scala:311)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1(KafkaMicroBatchStream.scala:203)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1$adapted(KafkaMicroBatchStream.scala:203)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$9(KafkaOffsetReaderAdmin.scala:508)\n",
      "\t\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\t\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderAdmin.scala:501)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:203)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:50)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.$anonfun$partitions$3(DataSourceV2ScanExecBase.scala:67)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions(DataSourceV2ScanExecBase.scala:67)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions$(DataSourceV2ScanExecBase.scala:66)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions(MicroBatchScanExec.scala:31)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD$lzycompute(MicroBatchScanExec.scala:56)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD(MicroBatchScanExec.scala:55)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute(DataSourceV2ScanExecBase.scala:187)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute$(DataSourceV2ScanExecBase.scala:185)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.doExecute(MicroBatchScanExec.scala:31)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:98)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec.doExecute(EventTimeWatermarkExec.scala:106)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:245)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:243)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:260)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:245)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:243)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:260)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 44 more\n",
      "25/08/29 11:52:48 ERROR MicroBatchExecution: Query [id = 7284ee0c-02ef-485e-a8c0-44a342c7fb32, runId = 09bab37a-c5d7-43af-830f-017735c2b9a8] terminated with error\n",
      "org.apache.spark.sql.kafka010.KafkaIllegalStateException: Some data may have been lost because they are not available in Kafka any more;\n",
      "either the data was aged out by Kafka or the topic may have been deleted before all the data in the\n",
      "topic was processed.\n",
      "If you don't want your streaming query to fail on such cases, set the source option failOnDataLoss to false.\n",
      "Reason: Partition stations-json-2 offset was changed from 453 to 33.\n",
      "\tat org.apache.spark.sql.kafka010.KafkaExceptions$.partitionOffsetChanged(KafkaExceptions.scala:151)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$10(KafkaOffsetReaderAdmin.scala:509)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.reportDataLoss(KafkaMicroBatchStream.scala:311)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1(KafkaMicroBatchStream.scala:203)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1$adapted(KafkaMicroBatchStream.scala:203)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$9(KafkaOffsetReaderAdmin.scala:508)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderAdmin.scala:501)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:203)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.$anonfun$partitions$3(DataSourceV2ScanExecBase.scala:67)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions(DataSourceV2ScanExecBase.scala:67)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions$(DataSourceV2ScanExecBase.scala:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions(MicroBatchScanExec.scala:31)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD$lzycompute(MicroBatchScanExec.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD(MicroBatchScanExec.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute(DataSourceV2ScanExecBase.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute$(DataSourceV2ScanExecBase.scala:185)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.doExecute(MicroBatchScanExec.scala:31)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec.doExecute(EventTimeWatermarkExec.scala:106)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:209)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:209)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:245)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:243)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:260)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$1(FileFormatWriter.scala:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:211)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:176)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:879)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaExceptions$.partitionOffsetChanged(KafkaExceptions.scala:151)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$10(KafkaOffsetReaderAdmin.scala:509)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.reportDataLoss(KafkaMicroBatchStream.scala:311)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1(KafkaMicroBatchStream.scala:203)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$planInputPartitions$1$adapted(KafkaMicroBatchStream.scala:203)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$getOffsetRangesFromResolvedOffsets$9(KafkaOffsetReaderAdmin.scala:508)\n",
      "\t\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\t\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderAdmin.scala:501)\n",
      "\t\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:203)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:50)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.$anonfun$partitions$3(DataSourceV2ScanExecBase.scala:67)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions(DataSourceV2ScanExecBase.scala:67)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.partitions$(DataSourceV2ScanExecBase.scala:66)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions(MicroBatchScanExec.scala:31)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD$lzycompute(MicroBatchScanExec.scala:56)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputRDD(MicroBatchScanExec.scala:55)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute(DataSourceV2ScanExecBase.scala:187)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.doExecute$(DataSourceV2ScanExecBase.scala:185)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.doExecute(MicroBatchScanExec.scala:31)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:98)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec.doExecute(EventTimeWatermarkExec.scala:106)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:245)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:243)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:260)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.doExecute(StreamingSymmetricHashJoinExec.scala:279)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:533)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:461)\n",
      "\t\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:460)\n",
      "\t\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:504)\n",
      "\t\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:761)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:201)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:260)\n",
      "\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:257)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:197)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:209)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:245)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:243)\n",
      "\t\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:260)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeRDD$1(SparkPlan.scala:188)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 44 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-03|3           |31.714536648388023|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-02|2           |19.41692640397019 |22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-02|2           |45.51282292449058 |46.6782408306873  |\n",
      "|D      |2000-01-01|2000-01-02|2           |13.693325465649643|16.533667605999277|\n",
      "|E      |2000-01-01|2000-01-02|2           |28.01508294119001 |28.619490102322693|\n",
      "|F      |2000-01-01|2000-01-02|2           |31.289931268409333|35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-02|2           |38.63036756812535 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-02|2           |34.27227636592002 |34.424849708703036|\n",
      "|I      |2000-01-01|2000-01-02|2           |36.863225085613024|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-02|2           |28.945549071181844|29.45454009738844 |\n",
      "|K      |2000-01-01|2000-01-02|2           |22.5250684211182  |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-02|2           |12.70189895741823 |16.859738358965586|\n",
      "|M      |2000-01-01|2000-01-02|2           |25.037209411355917|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-02|2           |32.47305338457275 |34.694409203376615|\n",
      "|O      |2000-01-01|2000-01-02|2           |25.959176403408378|27.213168259223593|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-03|3           |31.714536648388023|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-03|3           |18.865810571193062|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-03|3           |47.77427201865643 |52.29717020698815 |\n",
      "|D      |2000-01-01|2000-01-03|3           |13.643608611122508|16.533667605999277|\n",
      "|E      |2000-01-01|2000-01-03|3           |29.34043794162423 |31.99114794249268 |\n",
      "|F      |2000-01-01|2000-01-02|2           |31.289931268409333|35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-02|2           |38.63036756812535 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-02|2           |34.27227636592002 |34.424849708703036|\n",
      "|I      |2000-01-01|2000-01-02|2           |36.863225085613024|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-02|2           |28.945549071181844|29.45454009738844 |\n",
      "|K      |2000-01-01|2000-01-02|2           |22.5250684211182  |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-02|2           |12.70189895741823 |16.859738358965586|\n",
      "|M      |2000-01-01|2000-01-02|2           |25.037209411355917|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-02|2           |32.47305338457275 |34.694409203376615|\n",
      "|O      |2000-01-01|2000-01-02|2           |25.959176403408378|27.213168259223593|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, month, date_add, to_timestamp\n",
    "\n",
    "\n",
    "today = records.select(\"station\",\"date\",\"raining\") \\\n",
    "               .withColumn(\"event_time\", to_timestamp(col(\"date\"))) \\\n",
    "               .withWatermark(\"event_time\", \"3 days\")\n",
    "\n",
    "yesterday = records.select(\n",
    "                col(\"station\"),\n",
    "                date_add(col(\"date\"), 1).alias(\"date\"),\n",
    "                col(\"degrees\").alias(\"sub1degrees\"),\n",
    "                col(\"raining\").alias(\"sub1raining\")\n",
    "           ).withColumn(\"event_time\", to_timestamp(col(\"date\"))) \\\n",
    "            .withWatermark(\"event_time\", \"3 days\")\n",
    "\n",
    "two_days = records.select(\n",
    "                col(\"station\"),\n",
    "                date_add(col(\"date\"), 2).alias(\"date\"),\n",
    "                col(\"degrees\").alias(\"sub2degrees\"),\n",
    "                col(\"raining\").alias(\"sub2raining\")\n",
    "           ).withColumn(\"event_time\", to_timestamp(col(\"date\"))) \\\n",
    "            .withWatermark(\"event_time\", \"3 days\")\n",
    "\n",
    "\n",
    "training_rows = (\n",
    "    today\n",
    "      .join(\n",
    "          yesterday.select(\"station\",\"event_time\",\"sub1degrees\",\"sub1raining\"),\n",
    "          [\"station\",\"event_time\"], \"left\"\n",
    "      )\n",
    "      .join(\n",
    "          two_days.select(\"station\",\"event_time\",\"sub2degrees\",\"sub2raining\"),\n",
    "          [\"station\",\"event_time\"], \"left\"\n",
    "      )\n",
    "      .withColumn(\"month\", month(col(\"date\")))\n",
    "      .select(\"station\",\"date\",\"raining\",\"month\",\n",
    "              \"sub1degrees\",\"sub1raining\",\"sub2degrees\",\"sub2raining\")\n",
    ")\n",
    "\n",
    "\n",
    "out_path = \"rain_training_parquet\"\n",
    "ckpt     = \"chk_rain_training\"\n",
    "\n",
    "sink = (\n",
    "    training_rows.repartition(1)\n",
    "                 .writeStream\n",
    "                 .format(\"parquet\")\n",
    "                 .option(\"checkpointLocation\", ckpt)  \n",
    "                 .outputMode(\"append\")\n",
    "                 .trigger(processingTime=\"1 minute\")\n",
    "                 .start(out_path)                    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cea0a052-548b-4a85-b805-f121c546e7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-03|3           |31.714536648388023|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-03|3           |18.865810571193062|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-03|3           |47.77427201865643 |52.29717020698815 |\n",
      "|D      |2000-01-01|2000-01-03|3           |13.643608611122508|16.533667605999277|\n",
      "|E      |2000-01-01|2000-01-03|3           |29.34043794162423 |31.99114794249268 |\n",
      "|F      |2000-01-01|2000-01-03|3           |27.67662119949613 |35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-03|3           |38.33537804139353 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-03|3           |36.045604187859766|39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-03|3           |36.79379608088561 |39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-03|3           |29.64055439830615 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-02|2           |22.5250684211182  |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-02|2           |12.70189895741823 |16.859738358965586|\n",
      "|M      |2000-01-01|2000-01-02|2           |25.037209411355917|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-02|2           |32.47305338457275 |34.694409203376615|\n",
      "|O      |2000-01-01|2000-01-02|2           |25.959176403408378|27.213168259223593|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-03|3           |31.714536648388023|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-03|3           |18.865810571193062|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-03|3           |47.77427201865643 |52.29717020698815 |\n",
      "|D      |2000-01-01|2000-01-03|3           |13.643608611122508|16.533667605999277|\n",
      "|E      |2000-01-01|2000-01-03|3           |29.34043794162423 |31.99114794249268 |\n",
      "|F      |2000-01-01|2000-01-03|3           |27.67662119949613 |35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-03|3           |38.33537804139353 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-03|3           |36.045604187859766|39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-03|3           |36.79379608088561 |39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-03|3           |29.64055439830615 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-03|3           |22.21152571259724 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-03|3           |14.847154533107627|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-03|3           |22.569883928658765|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-03|3           |31.71153881986375 |34.694409203376615|\n",
      "|O      |2000-01-01|2000-01-03|3           |27.413302095503422|30.3215534796935  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-04|4           |31.264650048223924|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-04|4           |18.476028885189216|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-04|4           |49.398848303377484|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-04|4           |15.280967492443544|20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-03|3           |29.34043794162423 |31.99114794249268 |\n",
      "|F      |2000-01-01|2000-01-03|3           |27.67662119949613 |35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-03|3           |38.33537804139353 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-03|3           |36.045604187859766|39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-03|3           |36.79379608088561 |39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-03|3           |29.64055439830615 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-03|3           |22.21152571259724 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-03|3           |14.847154533107627|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-03|3           |22.569883928658765|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-03|3           |31.71153881986375 |34.694409203376615|\n",
      "|O      |2000-01-01|2000-01-03|3           |27.413302095503422|30.3215534796935  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for q in spark.streams.active:\n",
    "#     q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a6254f-11ab-49bb-90da-51b81f06da59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-04|4           |31.264650048223924|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-04|4           |18.476028885189216|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-04|4           |49.398848303377484|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-04|4           |15.280967492443544|20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-04|4           |30.555224419802784|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-04|4           |27.814902242197867|35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-04|4           |38.29102094216404 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-04|4           |34.48606310935395 |39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-04|4           |35.852413031207384|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-03|3           |29.64055439830615 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-03|3           |22.21152571259724 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-03|3           |14.847154533107627|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-03|3           |22.569883928658765|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-03|3           |31.71153881986375 |34.694409203376615|\n",
      "|O      |2000-01-01|2000-01-03|3           |27.413302095503422|30.3215534796935  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.regression import Regressor, RegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1f7fce9-e7d1-4369-8bd3-7bb6a27cc9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-04|4           |31.264650048223924|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-04|4           |18.476028885189216|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-04|4           |49.398848303377484|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-04|4           |15.280967492443544|20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-04|4           |30.555224419802784|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-04|4           |27.814902242197867|35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-04|4           |38.29102094216404 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-04|4           |34.48606310935395 |39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-04|4           |35.852413031207384|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-04|4           |29.608756838262707|31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-04|4           |21.59754710806139 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-04|4           |14.652985565271834|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-04|4           |23.100957133414575|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-04|4           |32.66168303592917 |35.512115684125426|\n",
      "|O      |2000-01-01|2000-01-03|3           |27.413302095503422|30.3215534796935  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(\"rain_training_parquet\")\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"month\",\"sub1degrees\",\"sub1raining\",\"sub2degrees\",\"sub2raining\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data  = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910f953a-c53f-4c4b-81b6-7e5da1cbc3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_e91afb6c63bc, depth=5, numNodes=45, numClasses=2, numFeatures=5\n",
      "  If (feature 2 <= 0.5)\n",
      "   If (feature 0 <= 3.5)\n",
      "    If (feature 3 <= 33.12982299395898)\n",
      "     If (feature 1 <= 40.304383232893066)\n",
      "      Predict: 0.0\n",
      "     Else (feature 1 > 40.304383232893066)\n",
      "      If (feature 0 <= 1.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 1.5)\n",
      "       Predict: 0.0\n",
      "    Else (feature 3 > 33.12982299395898)\n",
      "     If (feature 0 <= 2.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 0 > 2.5)\n",
      "      If (feature 1 <= 67.45038701134592)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 67.45038701134592)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3.5)\n",
      "    If (feature 1 <= 31.285077480407814)\n",
      "     If (feature 4 <= 0.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 4 > 0.5)\n",
      "      If (feature 0 <= 4.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 0 > 4.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 1 > 31.285077480407814)\n",
      "     If (feature 0 <= 4.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 0 > 4.5)\n",
      "      If (feature 1 <= 81.97926428811854)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 81.97926428811854)\n",
      "       Predict: 1.0\n",
      "  Else (feature 2 > 0.5)\n",
      "   If (feature 3 <= 46.22515275450209)\n",
      "    If (feature 3 <= 26.11765756470789)\n",
      "     Predict: 0.0\n",
      "    Else (feature 3 > 26.11765756470789)\n",
      "     If (feature 1 <= 36.696186808654716)\n",
      "      If (feature 3 <= 43.22031204409265)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 43.22031204409265)\n",
      "       Predict: 0.0\n",
      "     Else (feature 1 > 36.696186808654716)\n",
      "      If (feature 1 <= 48.708697868787496)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 48.708697868787496)\n",
      "       Predict: 0.0\n",
      "   Else (feature 3 > 46.22515275450209)\n",
      "    If (feature 1 <= 67.45038701134592)\n",
      "     If (feature 0 <= 3.5)\n",
      "      If (feature 1 <= 64.58278094843085)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 64.58278094843085)\n",
      "       Predict: 0.0\n",
      "     Else (feature 0 > 3.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 1 > 67.45038701134592)\n",
      "     If (feature 3 <= 77.3934363757692)\n",
      "      If (feature 3 <= 73.83126765717606)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 73.83126765717606)\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 > 77.3934363757692)\n",
      "      Predict: 1.0\n",
      "\n",
      "+------------------+-------------------+\n",
      "|avg(correct)      |avg(raining)       |\n",
      "+------------------+-------------------+\n",
      "|0.7474747474747475|0.31313131313131315|\n",
      "+------------------+-------------------+\n",
      "\n",
      "Accuracy: 0.7474747474747475\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-05|5           |30.61834414712188 |35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-05|5           |18.358609489587458|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-05|5           |49.842929277594536|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-04|4           |15.280967492443544|20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-04|4           |30.555224419802784|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-04|4           |27.814902242197867|35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-04|4           |38.29102094216404 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-04|4           |34.48606310935395 |39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-04|4           |35.852413031207384|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-04|4           |29.608756838262707|31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-04|4           |21.59754710806139 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-04|4           |14.652985565271834|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-04|4           |23.100957133414575|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-04|4           |32.66168303592917 |35.512115684125426|\n",
      "|O      |2000-01-01|2000-01-04|4           |28.858263160924118|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(out_path)\n",
    "\n",
    "# Drop early rows that dont yet have history\n",
    "data = data.na.drop(subset=[\"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"])\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"month\", \"sub1degrees\", \"sub1raining\", \"sub2degrees\", \"sub2raining\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "assembled = assembler.transform(data).select(\"station\", \"date\", \"raining\", \"features\")\n",
    "\n",
    "# Split & train\n",
    "train_data, test_data = assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"raining\", featuresCol=\"features\", maxDepth=5)\n",
    "dt_model = dt.fit(train_data)\n",
    "print(dt_model.toDebugString)\n",
    "\n",
    "# Evaluate\n",
    "pred = dt_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"raining\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "\n",
    "# Show the 2-number table like the spec\n",
    "pred.selectExpr(\n",
    "    \"double(prediction = raining) as correct\",\n",
    "    \"double(raining) as raining\"\n",
    ").groupBy().avg(\"correct\",\"raining\").show(truncate=False)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac973da-ff4d-432b-90dc-1361c1d108c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/29 11:53:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/5r/89jndgws4q39x08qhlxlm_gr0000gn/T/temporary-8e217583-988a-4c7f-9987-3121b0c38bd1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/29 11:53:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|K      |2000-01-04|0.0       |\n",
      "|C      |2000-01-03|0.0       |\n",
      "|H      |2000-01-03|0.0       |\n",
      "|H      |2000-01-04|0.0       |\n",
      "|M      |2000-01-04|0.0       |\n",
      "|N      |2000-01-03|0.0       |\n",
      "|N      |2000-01-04|0.0       |\n",
      "|B      |2000-01-03|0.0       |\n",
      "|L      |2000-01-03|0.0       |\n",
      "|E      |2000-01-04|0.0       |\n",
      "|B      |2000-01-04|0.0       |\n",
      "|C      |2000-01-04|0.0       |\n",
      "|D      |2000-01-03|0.0       |\n",
      "|G      |2000-01-04|0.0       |\n",
      "|D      |2000-01-05|0.0       |\n",
      "|I      |2000-01-03|0.0       |\n",
      "|J      |2000-01-03|0.0       |\n",
      "|C      |2000-01-05|0.0       |\n",
      "|E      |2000-01-03|0.0       |\n",
      "|G      |2000-01-03|0.0       |\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-05|5           |30.61834414712188 |35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-05|5           |18.358609489587458|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-05|5           |49.842929277594536|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-05|5           |14.67629261615736 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-05|5           |29.83084819959742 |34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-05|5           |29.34511942623078 |35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-05|5           |37.226186669942024|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-05|5           |34.06900071631736 |39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-04|4           |35.852413031207384|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-04|4           |29.608756838262707|31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-04|4           |21.59754710806139 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-04|4           |14.652985565271834|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-04|4           |23.100957133414575|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-04|4           |32.66168303592917 |35.512115684125426|\n",
      "|O      |2000-01-01|2000-01-04|4           |28.858263160924118|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|G      |2000-01-05|0.0       |\n",
      "|F      |2000-01-05|0.0       |\n",
      "|E      |2000-01-05|0.0       |\n",
      "|H      |2000-01-05|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-05|5           |30.61834414712188 |35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-05|5           |18.358609489587458|22.38389483821583 |\n",
      "|C      |2000-01-01|2000-01-05|5           |49.842929277594536|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-05|5           |14.67629261615736 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-05|5           |29.83084819959742 |34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-05|5           |29.34511942623078 |35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-05|5           |37.226186669942024|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-05|5           |34.06900071631736 |39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-05|5           |34.959849739642095|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-05|5           |27.45947627595469 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-05|5           |20.80766814713352 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-05|5           |13.913280832042707|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-05|5           |22.620598235020612|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-04|4           |32.66168303592917 |35.512115684125426|\n",
      "|O      |2000-01-01|2000-01-04|4           |28.858263160924118|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|M      |2000-01-05|0.0       |\n",
      "|I      |2000-01-05|0.0       |\n",
      "|K      |2000-01-05|0.0       |\n",
      "|L      |2000-01-05|0.0       |\n",
      "|J      |2000-01-05|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-06|6           |28.955777122700685|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-06|6           |19.062205469456792|22.580185368803473|\n",
      "|C      |2000-01-01|2000-01-05|5           |49.842929277594536|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-05|5           |14.67629261615736 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-05|5           |29.83084819959742 |34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-05|5           |29.34511942623078 |35.829869020157574|\n",
      "|G      |2000-01-01|2000-01-05|5           |37.226186669942024|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-05|5           |34.06900071631736 |39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-05|5           |34.959849739642095|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-05|5           |27.45947627595469 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-05|5           |20.80766814713352 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-05|5           |13.913280832042707|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-05|5           |22.620598235020612|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-05|5           |33.62192815867412 |37.462908649653926|\n",
      "|O      |2000-01-01|2000-01-05|5           |28.773230490897493|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|B      |2000-01-06|0.0       |\n",
      "|O      |2000-01-05|0.0       |\n",
      "|A      |2000-01-06|0.0       |\n",
      "|N      |2000-01-05|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-06|6           |28.955777122700685|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-06|6           |19.062205469456792|22.580185368803473|\n",
      "|C      |2000-01-01|2000-01-06|6           |48.4578689016524  |54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-06|6           |13.64824232610574 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-06|6           |30.099155007793055|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-06|6           |31.68779501162957 |43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-06|6           |35.415630485318694|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-05|5           |34.06900071631736 |39.592259831739256|\n",
      "|I      |2000-01-01|2000-01-05|5           |34.959849739642095|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-05|5           |27.45947627595469 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-05|5           |20.80766814713352 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-05|5           |13.913280832042707|19.137665684486418|\n",
      "|M      |2000-01-01|2000-01-05|5           |22.620598235020612|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-05|5           |33.62192815867412 |37.462908649653926|\n",
      "|O      |2000-01-01|2000-01-05|5           |28.773230490897493|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|F      |2000-01-06|0.0       |\n",
      "|G      |2000-01-06|0.0       |\n",
      "|E      |2000-01-06|0.0       |\n",
      "|C      |2000-01-06|0.0       |\n",
      "|D      |2000-01-06|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-06|6           |28.955777122700685|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-06|6           |19.062205469456792|22.580185368803473|\n",
      "|C      |2000-01-01|2000-01-06|6           |48.4578689016524  |54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-06|6           |13.64824232610574 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-06|6           |30.099155007793055|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-06|6           |31.68779501162957 |43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-06|6           |35.415630485318694|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-06|6           |35.4345670332074  |42.26239861765759 |\n",
      "|I      |2000-01-01|2000-01-06|6           |35.38129907651183 |39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-06|6           |24.72054040703837 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-06|6           |19.788025695546636|26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-06|6           |15.178343112060771|21.503654512151083|\n",
      "|M      |2000-01-01|2000-01-05|5           |22.620598235020612|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-05|5           |33.62192815867412 |37.462908649653926|\n",
      "|O      |2000-01-01|2000-01-05|5           |28.773230490897493|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|J      |2000-01-06|0.0       |\n",
      "|I      |2000-01-06|0.0       |\n",
      "|K      |2000-01-06|0.0       |\n",
      "|L      |2000-01-06|0.0       |\n",
      "|H      |2000-01-06|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-07|7           |27.119555404055664|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-06|6           |19.062205469456792|22.580185368803473|\n",
      "|C      |2000-01-01|2000-01-06|6           |48.4578689016524  |54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-06|6           |13.64824232610574 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-06|6           |30.099155007793055|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-06|6           |31.68779501162957 |43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-06|6           |35.415630485318694|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-06|6           |35.4345670332074  |42.26239861765759 |\n",
      "|I      |2000-01-01|2000-01-06|6           |35.38129907651183 |39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-06|6           |24.72054040703837 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-06|6           |19.788025695546636|26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-06|6           |15.178343112060771|21.503654512151083|\n",
      "|M      |2000-01-01|2000-01-06|6           |21.445721977262583|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-06|6           |33.47518216853798 |37.462908649653926|\n",
      "|O      |2000-01-01|2000-01-06|6           |29.225170805809366|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|O      |2000-01-06|0.0       |\n",
      "|M      |2000-01-06|0.0       |\n",
      "|N      |2000-01-06|0.0       |\n",
      "|A      |2000-01-07|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-07|7           |27.119555404055664|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-07|7           |19.29908842046486 |22.580185368803473|\n",
      "|C      |2000-01-01|2000-01-07|7           |48.185076796800445|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-07|7           |14.47142058640789 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-07|7           |29.672904679150925|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-07|7           |32.0830416660902  |43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-06|6           |35.415630485318694|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-06|6           |35.4345670332074  |42.26239861765759 |\n",
      "|I      |2000-01-01|2000-01-06|6           |35.38129907651183 |39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-06|6           |24.72054040703837 |31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-06|6           |19.788025695546636|26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-06|6           |15.178343112060771|21.503654512151083|\n",
      "|M      |2000-01-01|2000-01-06|6           |21.445721977262583|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-06|6           |33.47518216853798 |37.462908649653926|\n",
      "|O      |2000-01-01|2000-01-06|6           |29.225170805809366|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|B      |2000-01-07|0.0       |\n",
      "|C      |2000-01-07|0.0       |\n",
      "|D      |2000-01-07|0.0       |\n",
      "|F      |2000-01-07|0.0       |\n",
      "|E      |2000-01-07|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-07|7           |27.119555404055664|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-07|7           |19.29908842046486 |22.580185368803473|\n",
      "|C      |2000-01-01|2000-01-07|7           |48.185076796800445|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-07|7           |14.47142058640789 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-07|7           |29.672904679150925|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-07|7           |32.0830416660902  |43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-07|7           |34.41981870042679 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-07|7           |37.03209375795609 |46.61725410644822 |\n",
      "|I      |2000-01-01|2000-01-07|7           |34.403284067148356|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-07|7           |23.907374397678787|31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-07|7           |18.06474600363135 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-06|6           |15.178343112060771|21.503654512151083|\n",
      "|M      |2000-01-01|2000-01-06|6           |21.445721977262583|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-06|6           |33.47518216853798 |37.462908649653926|\n",
      "|O      |2000-01-01|2000-01-06|6           |29.225170805809366|33.1931463571862  |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|H      |2000-01-07|1.0       |\n",
      "|I      |2000-01-07|0.0       |\n",
      "|K      |2000-01-07|0.0       |\n",
      "|G      |2000-01-07|0.0       |\n",
      "|J      |2000-01-07|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-07|7           |27.119555404055664|35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-07|7           |19.29908842046486 |22.580185368803473|\n",
      "|C      |2000-01-01|2000-01-07|7           |48.185076796800445|54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-07|7           |14.47142058640789 |20.193044136406655|\n",
      "|E      |2000-01-01|2000-01-07|7           |29.672904679150925|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-07|7           |32.0830416660902  |43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-07|7           |34.41981870042679 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-07|7           |37.03209375795609 |46.61725410644822 |\n",
      "|I      |2000-01-01|2000-01-07|7           |34.403284067148356|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-07|7           |23.907374397678787|31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-07|7           |18.06474600363135 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-07|7           |15.230562658206193|21.503654512151083|\n",
      "|M      |2000-01-01|2000-01-07|7           |19.795569034577152|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-07|7           |34.58248768791355 |41.226320804166996|\n",
      "|O      |2000-01-01|2000-01-07|7           |31.116051954655347|42.461338847731234|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|N      |2000-01-07|0.0       |\n",
      "|M      |2000-01-07|0.0       |\n",
      "|L      |2000-01-07|0.0       |\n",
      "|O      |2000-01-07|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-08|8           |25.01017618869044 |35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-08|8           |19.870560278788   |23.870863287049993|\n",
      "|C      |2000-01-01|2000-01-08|8           |46.93151179639949 |54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-08|8           |16.150943735396712|27.907605778318466|\n",
      "|E      |2000-01-01|2000-01-08|8           |28.124542704156582|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-07|7           |32.0830416660902  |43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-07|7           |34.41981870042679 |42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-07|7           |37.03209375795609 |46.61725410644822 |\n",
      "|I      |2000-01-01|2000-01-07|7           |34.403284067148356|39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-07|7           |23.907374397678787|31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-07|7           |18.06474600363135 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-07|7           |15.230562658206193|21.503654512151083|\n",
      "|M      |2000-01-01|2000-01-07|7           |19.795569034577152|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-07|7           |34.58248768791355 |41.226320804166996|\n",
      "|O      |2000-01-01|2000-01-07|7           |31.116051954655347|42.461338847731234|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|B      |2000-01-08|0.0       |\n",
      "|E      |2000-01-08|0.0       |\n",
      "|C      |2000-01-08|0.0       |\n",
      "|A      |2000-01-08|0.0       |\n",
      "|D      |2000-01-08|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-01-08|8           |25.01017618869044 |35.99886859506166 |\n",
      "|B      |2000-01-01|2000-01-08|8           |19.870560278788   |23.870863287049993|\n",
      "|C      |2000-01-01|2000-01-08|8           |46.93151179639949 |54.27257715754064 |\n",
      "|D      |2000-01-01|2000-01-08|8           |16.150943735396712|27.907605778318466|\n",
      "|E      |2000-01-01|2000-01-08|8           |28.124542704156582|34.19958385433844 |\n",
      "|F      |2000-01-01|2000-01-08|8           |33.018901365562485|43.4011729386235  |\n",
      "|G      |2000-01-01|2000-01-08|8           |32.596110505836194|42.352589804873205|\n",
      "|H      |2000-01-01|2000-01-08|8           |39.144849665200496|53.934141015911344|\n",
      "|I      |2000-01-01|2000-01-08|8           |33.46853124250586 |39.361349383317425|\n",
      "|J      |2000-01-01|2000-01-08|8           |22.908633948624452|31.03056505255476 |\n",
      "|K      |2000-01-01|2000-01-07|7           |18.06474600363135 |26.218568370747775|\n",
      "|L      |2000-01-01|2000-01-07|7           |15.230562658206193|21.503654512151083|\n",
      "|M      |2000-01-01|2000-01-07|7           |19.795569034577152|26.777610600419806|\n",
      "|N      |2000-01-01|2000-01-07|7           |34.58248768791355 |41.226320804166996|\n",
      "|O      |2000-01-01|2000-01-07|7           |31.116051954655347|42.461338847731234|\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|date      |prediction|\n",
      "+-------+----------+----------+\n",
      "|H      |2000-01-08|0.0       |\n",
      "|I      |2000-01-08|0.0       |\n",
      "|G      |2000-01-08|0.0       |\n",
      "|F      |2000-01-08|0.0       |\n",
      "|J      |2000-01-08|0.0       |\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_stream = training_rows.select(\n",
    "    \"station\",\"date\",\"month\",\"sub1degrees\",\"sub1raining\",\"sub2degrees\",\"sub2raining\"\n",
    ")\n",
    "\n",
    "# Drop rows that dont have full history yet\n",
    "features_stream_clean = features_stream.na.drop(\n",
    "    subset=[\"sub1degrees\",\"sub1raining\",\"sub2degrees\",\"sub2raining\"]\n",
    ")\n",
    "\n",
    "# Assemble with skip just in case\n",
    "assembler_pred = VectorAssembler(\n",
    "    inputCols=[\"month\",\"sub1degrees\",\"sub1raining\",\"sub2degrees\",\"sub2raining\"],\n",
    "    outputCol=\"features\"\n",
    ").setHandleInvalid(\"skip\")\n",
    "\n",
    "features_for_pred = assembler_pred.transform(features_stream_clean).select(\"station\",\"date\",\"features\")\n",
    "\n",
    "pred_stream = dt_model.transform(features_for_pred).select(\"station\",\"date\",\"prediction\")\n",
    "\n",
    "# (optional) filter to one station\n",
    "# pred_stream = pred_stream.filter(\"station = 'A'\")\n",
    "\n",
    "q_pred = (pred_stream.writeStream\n",
    "          .format(\"console\")\n",
    "          .outputMode(\"append\")\n",
    "          .option(\"truncate\",\"false\")\n",
    "          .trigger(processingTime=\"5 seconds\")\n",
    "          .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f75c6c9-a0f5-4bbb-b52c-19d3eff1acc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 12:16:39 WARN DAGScheduler: Failed to cancel job group 4c31426c-c1d5-487e-9d69-484b79bfe2c3. Cannot find active jobs for it.\n",
      "25/08/27 12:16:39 WARN DAGScheduler: Failed to cancel job group 4c31426c-c1d5-487e-9d69-484b79bfe2c3. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "for q in spark.streams.active: q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db66cf91-d35c-474e-915e-e8b0548e9aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 32\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-08|68          |31.652024715046924|53.43912012050109|\n",
      "|B      |2000-01-01|2000-03-08|68          |22.870967653965998|44.40074419586528|\n",
      "|C      |2000-01-01|2000-03-08|68          |16.55702984241322 |42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-08|68          |22.607706280882088|38.24475903129374|\n",
      "|E      |2000-01-01|2000-03-08|68          |49.13944375751521 |67.26244762498868|\n",
      "|F      |2000-01-01|2000-03-07|67          |22.825837370913117|41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-07|67          |36.305611378090894|65.74825341925056|\n",
      "|H      |2000-01-01|2000-03-07|67          |30.19538446833792 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-07|67          |26.782462628026643|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-07|67          |22.166144139263032|43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-07|67          |34.06767515596985 |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-07|67          |34.711220587644156|52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-07|67          |38.9194253513435  |61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-07|67          |17.02031049242088 |37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-07|67          |19.83728483331191 |42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 33\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-08|68          |31.652024715046924|53.43912012050109|\n",
      "|B      |2000-01-01|2000-03-08|68          |22.870967653965998|44.40074419586528|\n",
      "|C      |2000-01-01|2000-03-08|68          |16.55702984241322 |42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-08|68          |22.607706280882088|38.24475903129374|\n",
      "|E      |2000-01-01|2000-03-08|68          |49.13944375751521 |67.26244762498868|\n",
      "|F      |2000-01-01|2000-03-08|68          |22.76239127208542 |41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-08|68          |36.79721729723486 |69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-08|68          |30.41166347422215 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-08|68          |27.045697738968776|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-08|68          |22.18543661717166 |43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-07|67          |34.06767515596985 |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-07|67          |34.711220587644156|52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-07|67          |38.9194253513435  |61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-07|67          |17.02031049242088 |37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-07|67          |19.83728483331191 |42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 34\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-08|68          |31.652024715046924|53.43912012050109|\n",
      "|B      |2000-01-01|2000-03-08|68          |22.870967653965998|44.40074419586528|\n",
      "|C      |2000-01-01|2000-03-08|68          |16.55702984241322 |42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-08|68          |22.607706280882088|38.24475903129374|\n",
      "|E      |2000-01-01|2000-03-08|68          |49.13944375751521 |67.26244762498868|\n",
      "|F      |2000-01-01|2000-03-08|68          |22.76239127208542 |41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-08|68          |36.79721729723486 |69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-08|68          |30.41166347422215 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-08|68          |27.045697738968776|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-08|68          |22.18543661717166 |43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-08|68          |34.1991288793792  |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-08|68          |34.94286320471002 |52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-08|68          |38.966287184935275|61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-08|68          |17.209617757813   |37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-08|68          |20.068797942557573|42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 35\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-09|69          |32.006146619720965|56.08643613755558|\n",
      "|B      |2000-01-01|2000-03-09|69          |23.128047655035147|44.40074419586528|\n",
      "|C      |2000-01-01|2000-03-09|69          |16.867101790476834|42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-09|69          |22.961901991215846|47.04721029391135|\n",
      "|E      |2000-01-01|2000-03-08|68          |49.13944375751521 |67.26244762498868|\n",
      "|F      |2000-01-01|2000-03-08|68          |22.76239127208542 |41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-08|68          |36.79721729723486 |69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-08|68          |30.41166347422215 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-08|68          |27.045697738968776|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-08|68          |22.18543661717166 |43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-08|68          |34.1991288793792  |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-08|68          |34.94286320471002 |52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-08|68          |38.966287184935275|61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-08|68          |17.209617757813   |37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-08|68          |20.068797942557573|42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 36\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-09|69          |32.006146619720965|56.08643613755558|\n",
      "|B      |2000-01-01|2000-03-09|69          |23.128047655035147|44.40074419586528|\n",
      "|C      |2000-01-01|2000-03-09|69          |16.867101790476834|42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-09|69          |22.961901991215846|47.04721029391135|\n",
      "|E      |2000-01-01|2000-03-09|69          |49.48961218453843 |73.30106522211727|\n",
      "|F      |2000-01-01|2000-03-09|69          |22.667330600834617|41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-09|69          |37.22978489797649 |69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-09|69          |30.55570107933952 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-09|69          |27.280718187121806|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-08|68          |22.18543661717166 |43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-08|68          |34.1991288793792  |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-08|68          |34.94286320471002 |52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-08|68          |38.966287184935275|61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-08|68          |17.209617757813   |37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-08|68          |20.068797942557573|42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 37\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-09|69          |32.006146619720965|56.08643613755558|\n",
      "|B      |2000-01-01|2000-03-09|69          |23.128047655035147|44.40074419586528|\n",
      "|C      |2000-01-01|2000-03-09|69          |16.867101790476834|42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-09|69          |22.961901991215846|47.04721029391135|\n",
      "|E      |2000-01-01|2000-03-09|69          |49.48961218453843 |73.30106522211727|\n",
      "|F      |2000-01-01|2000-03-09|69          |22.667330600834617|41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-09|69          |37.22978489797649 |69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-09|69          |30.55570107933952 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-09|69          |27.280718187121806|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-09|69          |22.360584442507164|43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-09|69          |34.40861428181699 |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-09|69          |35.17056808740801 |52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-09|69          |39.027796972898486|61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-09|69          |17.320422296238206|37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-08|68          |20.068797942557573|42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 38\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-10|70          |32.21892450578158 |56.08643613755558|\n",
      "|B      |2000-01-01|2000-03-10|70          |23.48633132513641 |48.2079045621235 |\n",
      "|C      |2000-01-01|2000-03-10|70          |17.142847855851866|42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-09|69          |22.961901991215846|47.04721029391135|\n",
      "|E      |2000-01-01|2000-03-09|69          |49.48961218453843 |73.30106522211727|\n",
      "|F      |2000-01-01|2000-03-09|69          |22.667330600834617|41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-09|69          |37.22978489797649 |69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-09|69          |30.55570107933952 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-09|69          |27.280718187121806|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-09|69          |22.360584442507164|43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-09|69          |34.40861428181699 |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-09|69          |35.17056808740801 |52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-09|69          |39.027796972898486|61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-09|69          |17.320422296238206|37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-09|69          |20.38923665935688 |42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 39\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-10|70          |32.21892450578158 |56.08643613755558|\n",
      "|B      |2000-01-01|2000-03-10|70          |23.48633132513641 |48.2079045621235 |\n",
      "|C      |2000-01-01|2000-03-10|70          |17.142847855851866|42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-10|70          |23.25121376189651 |47.04721029391135|\n",
      "|E      |2000-01-01|2000-03-10|70          |49.81439556627941 |73.30106522211727|\n",
      "|F      |2000-01-01|2000-03-10|70          |22.733246404865795|41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-10|70          |37.636626185521095|69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-10|70          |30.65258431729765 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-09|69          |27.280718187121806|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-09|69          |22.360584442507164|43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-09|69          |34.40861428181699 |51.79220547150891|\n",
      "|L      |2000-01-01|2000-03-09|69          |35.17056808740801 |52.21313226541907|\n",
      "|M      |2000-01-01|2000-03-09|69          |39.027796972898486|61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-09|69          |17.320422296238206|37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-09|69          |20.38923665935688 |42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 40\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|station|start     |end       |measurements|avg               |max              |\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "|A      |2000-01-01|2000-03-10|70          |32.21892450578158 |56.08643613755558|\n",
      "|B      |2000-01-01|2000-03-10|70          |23.48633132513641 |48.2079045621235 |\n",
      "|C      |2000-01-01|2000-03-10|70          |17.142847855851866|42.56271453750142|\n",
      "|D      |2000-01-01|2000-03-10|70          |23.25121376189651 |47.04721029391135|\n",
      "|E      |2000-01-01|2000-03-10|70          |49.81439556627941 |73.30106522211727|\n",
      "|F      |2000-01-01|2000-03-10|70          |22.733246404865795|41.65781054832314|\n",
      "|G      |2000-01-01|2000-03-10|70          |37.636626185521095|69.73481387988076|\n",
      "|H      |2000-01-01|2000-03-10|70          |30.65258431729765 |50.92274448232121|\n",
      "|I      |2000-01-01|2000-03-10|70          |27.383435262029522|49.92958933701394|\n",
      "|J      |2000-01-01|2000-03-10|70          |22.418240721519652|43.48620935044129|\n",
      "|K      |2000-01-01|2000-03-10|70          |34.6865103895031  |53.86134181984462|\n",
      "|L      |2000-01-01|2000-03-10|70          |35.49273305074616 |57.72211552107861|\n",
      "|M      |2000-01-01|2000-03-10|70          |39.204660721571095|61.87182934202819|\n",
      "|N      |2000-01-01|2000-03-09|69          |17.320422296238206|37.79395709681478|\n",
      "|O      |2000-01-01|2000-03-09|69          |20.38923665935688 |42.44905627140259|\n",
      "+-------+----------+----------+------------+------------------+-----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 41\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-03-11|71          |32.5649673712656  |56.78796795514765 |\n",
      "|B      |2000-01-01|2000-03-11|71          |23.892310541752316|52.310855704865936|\n",
      "|C      |2000-01-01|2000-03-10|70          |17.142847855851866|42.56271453750142 |\n",
      "|D      |2000-01-01|2000-03-10|70          |23.25121376189651 |47.04721029391135 |\n",
      "|E      |2000-01-01|2000-03-10|70          |49.81439556627941 |73.30106522211727 |\n",
      "|F      |2000-01-01|2000-03-10|70          |22.733246404865795|41.65781054832314 |\n",
      "|G      |2000-01-01|2000-03-10|70          |37.636626185521095|69.73481387988076 |\n",
      "|H      |2000-01-01|2000-03-10|70          |30.65258431729765 |50.92274448232121 |\n",
      "|I      |2000-01-01|2000-03-10|70          |27.383435262029522|49.92958933701394 |\n",
      "|J      |2000-01-01|2000-03-10|70          |22.418240721519652|43.48620935044129 |\n",
      "|K      |2000-01-01|2000-03-10|70          |34.6865103895031  |53.86134181984462 |\n",
      "|L      |2000-01-01|2000-03-10|70          |35.49273305074616 |57.72211552107861 |\n",
      "|M      |2000-01-01|2000-03-10|70          |39.204660721571095|61.87182934202819 |\n",
      "|N      |2000-01-01|2000-03-10|70          |17.46041846194643 |37.79395709681478 |\n",
      "|O      |2000-01-01|2000-03-10|70          |20.5849932789889  |42.44905627140259 |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 42\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-03-11|71          |32.5649673712656  |56.78796795514765 |\n",
      "|B      |2000-01-01|2000-03-11|71          |23.892310541752316|52.310855704865936|\n",
      "|C      |2000-01-01|2000-03-11|71          |17.442888571777292|42.56271453750142 |\n",
      "|D      |2000-01-01|2000-03-11|71          |23.631780998833175|50.27148758439992 |\n",
      "|E      |2000-01-01|2000-03-11|71          |50.18509317764817 |76.13392597346132 |\n",
      "|F      |2000-01-01|2000-03-11|71          |22.834298559903004|41.65781054832314 |\n",
      "|G      |2000-01-01|2000-03-11|71          |37.858607303517246|69.73481387988076 |\n",
      "|H      |2000-01-01|2000-03-10|70          |30.65258431729765 |50.92274448232121 |\n",
      "|I      |2000-01-01|2000-03-10|70          |27.383435262029522|49.92958933701394 |\n",
      "|J      |2000-01-01|2000-03-10|70          |22.418240721519652|43.48620935044129 |\n",
      "|K      |2000-01-01|2000-03-10|70          |34.6865103895031  |53.86134181984462 |\n",
      "|L      |2000-01-01|2000-03-10|70          |35.49273305074616 |57.72211552107861 |\n",
      "|M      |2000-01-01|2000-03-10|70          |39.204660721571095|61.87182934202819 |\n",
      "|N      |2000-01-01|2000-03-10|70          |17.46041846194643 |37.79395709681478 |\n",
      "|O      |2000-01-01|2000-03-10|70          |20.5849932789889  |42.44905627140259 |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 43\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-03-11|71          |32.5649673712656  |56.78796795514765 |\n",
      "|B      |2000-01-01|2000-03-11|71          |23.892310541752316|52.310855704865936|\n",
      "|C      |2000-01-01|2000-03-11|71          |17.442888571777292|42.56271453750142 |\n",
      "|D      |2000-01-01|2000-03-11|71          |23.631780998833175|50.27148758439992 |\n",
      "|E      |2000-01-01|2000-03-11|71          |50.18509317764817 |76.13392597346132 |\n",
      "|F      |2000-01-01|2000-03-11|71          |22.834298559903004|41.65781054832314 |\n",
      "|G      |2000-01-01|2000-03-11|71          |37.858607303517246|69.73481387988076 |\n",
      "|H      |2000-01-01|2000-03-11|71          |30.868329301855265|50.92274448232121 |\n",
      "|I      |2000-01-01|2000-03-11|71          |27.522613449267027|49.92958933701394 |\n",
      "|J      |2000-01-01|2000-03-11|71          |22.602954050880065|43.48620935044129 |\n",
      "|K      |2000-01-01|2000-03-11|71          |34.86793511140542 |53.86134181984462 |\n",
      "|L      |2000-01-01|2000-03-11|71          |35.79481154394376 |57.72211552107861 |\n",
      "|M      |2000-01-01|2000-03-10|70          |39.204660721571095|61.87182934202819 |\n",
      "|N      |2000-01-01|2000-03-10|70          |17.46041846194643 |37.79395709681478 |\n",
      "|O      |2000-01-01|2000-03-10|70          |20.5849932789889  |42.44905627140259 |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 44\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-03-11|71          |32.5649673712656  |56.78796795514765 |\n",
      "|B      |2000-01-01|2000-03-11|71          |23.892310541752316|52.310855704865936|\n",
      "|C      |2000-01-01|2000-03-11|71          |17.442888571777292|42.56271453750142 |\n",
      "|D      |2000-01-01|2000-03-11|71          |23.631780998833175|50.27148758439992 |\n",
      "|E      |2000-01-01|2000-03-11|71          |50.18509317764817 |76.13392597346132 |\n",
      "|F      |2000-01-01|2000-03-11|71          |22.834298559903004|41.65781054832314 |\n",
      "|G      |2000-01-01|2000-03-11|71          |37.858607303517246|69.73481387988076 |\n",
      "|H      |2000-01-01|2000-03-11|71          |30.868329301855265|50.92274448232121 |\n",
      "|I      |2000-01-01|2000-03-11|71          |27.522613449267027|49.92958933701394 |\n",
      "|J      |2000-01-01|2000-03-11|71          |22.602954050880065|43.48620935044129 |\n",
      "|K      |2000-01-01|2000-03-11|71          |34.86793511140542 |53.86134181984462 |\n",
      "|L      |2000-01-01|2000-03-11|71          |35.79481154394376 |57.72211552107861 |\n",
      "|M      |2000-01-01|2000-03-11|71          |39.33209274352589 |61.87182934202819 |\n",
      "|N      |2000-01-01|2000-03-11|71          |17.556145895248164|37.79395709681478 |\n",
      "|O      |2000-01-01|2000-03-11|71          |20.81557532057516 |42.44905627140259 |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 45\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|station|start     |end       |measurements|avg               |max               |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "|A      |2000-01-01|2000-03-12|72          |32.80119232595857 |56.78796795514765 |\n",
      "|B      |2000-01-01|2000-03-12|72          |24.25885242449372 |52.310855704865936|\n",
      "|C      |2000-01-01|2000-03-12|72          |17.653616326456934|42.56271453750142 |\n",
      "|D      |2000-01-01|2000-03-12|72          |23.905891219991   |50.27148758439992 |\n",
      "|E      |2000-01-01|2000-03-12|72          |50.62796165812805 |82.07162377220013 |\n",
      "|F      |2000-01-01|2000-03-11|71          |22.834298559903004|41.65781054832314 |\n",
      "|G      |2000-01-01|2000-03-11|71          |37.858607303517246|69.73481387988076 |\n",
      "|H      |2000-01-01|2000-03-11|71          |30.868329301855265|50.92274448232121 |\n",
      "|I      |2000-01-01|2000-03-11|71          |27.522613449267027|49.92958933701394 |\n",
      "|J      |2000-01-01|2000-03-11|71          |22.602954050880065|43.48620935044129 |\n",
      "|K      |2000-01-01|2000-03-11|71          |34.86793511140542 |53.86134181984462 |\n",
      "|L      |2000-01-01|2000-03-11|71          |35.79481154394376 |57.72211552107861 |\n",
      "|M      |2000-01-01|2000-03-11|71          |39.33209274352589 |61.87182934202819 |\n",
      "|N      |2000-01-01|2000-03-11|71          |17.556145895248164|37.79395709681478 |\n",
      "|O      |2000-01-01|2000-03-11|71          |20.81557532057516 |42.44905627140259 |\n",
      "+-------+----------+----------+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_pred.isActive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2378300-75dc-462c-91ca-4ee6fea6a4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
